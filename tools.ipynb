{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362312fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4901251",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-29T10:30:12.767665Z",
     "iopub.status.busy": "2023-03-29T10:30:12.767166Z",
     "iopub.status.idle": "2023-03-29T10:30:15.992560Z",
     "shell.execute_reply": "2023-03-29T10:30:15.991664Z",
     "shell.execute_reply.started": "2023-03-29T10:30:12.767643Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6aa7c9e3c5cb0906",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from os import listdir\n",
    "import os\n",
    "import wfdb\n",
    "import random\n",
    "import datetime\n",
    "import time\n",
    "import itertools\n",
    "import pickle\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler, RobustScaler\n",
    "from tempfile import TemporaryFile\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM, GRU\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Embedding\n",
    "\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, roc_curve\n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import LeaveOneGroupOut, GroupShuffleSplit\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a96a5977",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-29T10:30:46.059419Z",
     "iopub.status.busy": "2023-03-29T10:30:46.058604Z",
     "iopub.status.idle": "2023-03-29T10:30:46.076041Z",
     "shell.execute_reply": "2023-03-29T10:30:46.075236Z",
     "shell.execute_reply.started": "2023-03-29T10:30:46.059361Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from matplotlib import pyplot\n",
    "\n",
    "def losscurve(historyv, name='losscurve'):\n",
    "    loss = historyv.history['loss']\n",
    "    val_loss = historyv.history['val_loss']\n",
    "    epochs = range(1, len(loss) + 1)\n",
    "    plt.plot(epochs, loss, 'y', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.savefig(name)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def acccurve(historyv, name='acccurve'):\n",
    "    acc = historyv.history['accuracy']\n",
    "    val_acc = historyv.history['val_accuracy']\n",
    "    loss = historyv.history['loss']\n",
    "    epochs = range(1, len(loss) + 1)\n",
    "    plt.plot(epochs, acc, 'y', label='Training acc')\n",
    "    plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.savefig(name)\n",
    "    plt.show()\n",
    "\n",
    "def ROC_curve(test, pred_pr, name='ROC_curve'):\n",
    "    auc = roc_auc_score(test, pred_pr)\n",
    "    fpr, tpr, thresholds = roc_curve(test, pred_pr)\n",
    "    \n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot([0, 1], [0, 1], 'k--', linewidth=3) # dashed line with black(k) color\n",
    "\n",
    "    plt.plot(fpr, tpr, label='AUC = %0.4f' % auc, linewidth=3)\n",
    "\n",
    "    plt.xlabel('False positive rate', fontsize=18)\n",
    "    plt.ylabel('True positive rate', fontsize=18)\n",
    "    plt.ylabel('True positive rate', fontsize=18)\n",
    "    plt.xticks(fontsize=16)\n",
    "    plt.yticks(fontsize=16)\n",
    "    plt.title('ROC curve', fontsize=18)\n",
    "    plt.legend(loc='best', fontsize=18)\n",
    "    plt.savefig(name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "802f5e6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-29T10:30:47.832511Z",
     "iopub.status.busy": "2023-03-29T10:30:47.831434Z",
     "iopub.status.idle": "2023-03-29T10:30:47.846744Z",
     "shell.execute_reply": "2023-03-29T10:30:47.845936Z",
     "shell.execute_reply.started": "2023-03-29T10:30:47.832450Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = (cm.astype('float') / cm.sum(axis=1)[:, np.newaxis])*100\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, fontsize=14)\n",
    "    #plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45, fontsize=14)\n",
    "    plt.yticks(tick_marks, classes, fontsize=14)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 fontsize=14,\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.grid(False)\n",
    "    plt.ylabel('True label', fontsize=14)\n",
    "    plt.xlabel('Predicted label',fontsize=14)\n",
    "    plt.savefig(title, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8709c63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-29T10:30:48.989165Z",
     "iopub.status.busy": "2023-03-29T10:30:48.988361Z",
     "iopub.status.idle": "2023-03-29T10:30:49.006153Z",
     "shell.execute_reply": "2023-03-29T10:30:49.005187Z",
     "shell.execute_reply.started": "2023-03-29T10:30:48.989104Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(X_test,y_test, model, modelname,time):\n",
    "    #acc = model.score(X_test, y_test)\n",
    "    #results = model.evaluate(X_test, y_test)\n",
    "    #acc_r = results[1]\n",
    "    #print(acc_r)\n",
    "    y_pred_pr = model.predict(X_test)\n",
    "    y_pred_cl = np.argmax(y_pred_pr, axis=1)\n",
    "    print(y_pred_pr[:10])\n",
    "    print('using probability for having AFIB') \n",
    "    y_pred_pr = y_pred_pr[:,1]\n",
    "    print(y_pred_pr[:10])\n",
    "   \n",
    "    print('y_pred_cl')\n",
    "    print(y_pred_cl[:10])   \n",
    "    #y_pred = np.argmax(y_pred, axis=1)\n",
    "    \n",
    "    print(y_test[:10]) \n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "    print('y_test')\n",
    "    print(y_test[:10]) \n",
    "    \n",
    "    ##save predictions and labels to file\n",
    "    np.save('y_pred', y_pred_pr)\n",
    "    np.save('y_test', y_test)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(list(y_test), list(y_pred_cl), labels=[0, 1]).ravel()\n",
    "    cm = confusion_matrix(list(y_test), list(y_pred_cl), labels=[0, 1])\n",
    "    #Sensitivity/Recall/TPR\n",
    "    #tpr= tp/(fn+tp)\n",
    "    tpr_w = recall_score(y_test, y_pred_cl, average='weighted')\n",
    "    tpr = recall_score(y_test, y_pred_cl)\n",
    "\n",
    "    #Specificity/TNR\n",
    "    tnr = tn/(tn+fp)\n",
    "    #Precision/Positive Predictive value (weighted)\n",
    "    #prec = tp/(tp+fp)\n",
    "    prec = precision_score(y_test, y_pred_cl, average='weighted')\n",
    "    #FPR\n",
    "    fpr = fp/(tn+fp)\n",
    "    #Accuracy\n",
    "    acc = (tp+tn)/(tp+tn+fp+fn)\n",
    "    try:\n",
    "        roc_auc = roc_auc_score(y_test, y_pred_pr)\n",
    "        ROC_curve(y_test, y_pred_pr, name='ROC_curve')\n",
    "    except ValueError:\n",
    "        #pass\n",
    "        roc_auc = np.nan    \n",
    "    \n",
    "    \"\"\"save confusion matrix as dataframe\"\"\"\n",
    "    confusionDF = pd.DataFrame({'tn': [tn] ,'fp':[fp], 'fn':[fn], 'tp':[tp]}, \n",
    "                           columns=['tn','fp', 'fn', 'tp'])\n",
    "    confusionDF.to_pickle('confusionDF.pkl')\n",
    "\n",
    "    # non-normalized confusion matrix\n",
    "    plot_confusion_matrix(cm, classes=['Normal','AFIB'],\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues)\n",
    "    # normalized confusion matrix\n",
    "    plot_confusion_matrix(cm, classes=['Normal','AFIB'],\n",
    "                          normalize=True,\n",
    "                          title='Confusion matrix_norm',\n",
    "                          cmap=plt.cm.Blues)\n",
    "    \n",
    "    f1 = f1_score(y_test, y_pred_cl, average='weighted')\n",
    "\n",
    "    toappend = pd.DataFrame({'model': [modelname] ,'acc':[acc], 'tpr':[tpr], 'tnr':[tnr], 'prec':[prec],'fpr':[fpr], 'roc_auc':[roc_auc], 'f1':[f1], 'time': [time]}, \n",
    "                           columns=['model','acc', 'tpr', 'tnr', 'prec','fpr','roc_auc','f1', 'time'])\n",
    "    \n",
    "    return toappend\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "addfd095",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-29T10:59:29.929397Z",
     "iopub.status.busy": "2023-03-29T10:59:29.928593Z",
     "iopub.status.idle": "2023-03-29T10:59:29.947590Z",
     "shell.execute_reply": "2023-03-29T10:59:29.946691Z",
     "shell.execute_reply.started": "2023-03-29T10:59:29.929336Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def split_train_eval(model, epochsN,BATCH=32,SHUFFLE=True,SPLITS=1):\n",
    "\n",
    "    metrics = pd.DataFrame({'model':[],'acc':[], 'tpr':[], 'tnr':[], 'prec':[], 'fpr':[], 'roc_auc':[], 'f1':[],'time':[]},\n",
    "                        columns=['model','acc', 'tpr', 'tnr', 'prec','fpr','roc_auc','f1','time'])\n",
    "    \n",
    "    \"\"\"change seed to 40 from 42\"\"\"\n",
    "    gss = GroupShuffleSplit(n_splits=SPLITS, train_size=.7, random_state=42)\n",
    "    \n",
    "    for train_index, test_index in gss.split(X, y, ids):\n",
    "    \n",
    "        X_train = np.array(list(itemgetter(*train_index)(X)))\n",
    "        X_test = np.array(list(itemgetter(*test_index)(X)))\n",
    "        y_train = np.array(list(itemgetter(*train_index)(y)))\n",
    "        y_test = np.array(list(itemgetter(*test_index)(y)))\n",
    "    \n",
    "        print(f\"shape of x_train: {np.shape(X_train)}, type: {type(X_train)}\")\n",
    "        print(f\"shape of x_test: {np.shape(X_test)}, type: {type(X_test)}\")\n",
    "        \n",
    "        \n",
    "        if (len(X_train)+len(X_test) != len(X)):\n",
    "            raise TypeError(\"train and test sets don't add up!\")\n",
    "        \n",
    "        #get time for start of training. Used in evaluate()\n",
    "        start = time.time()\n",
    "        \n",
    "        model.compile(loss='binary_crossentropy',\n",
    "            optimizer='adam', metrics = ['accuracy'])\n",
    "\n",
    "        history = model.fit(X_train, y_train,    \n",
    "                    epochs=epochsN,\n",
    "                    batch_size = BATCH,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    shuffle=SHUFFLE,        \n",
    "                    verbose=1)\n",
    "        print(\"fitted\")\n",
    "        pickle.dump(model, open('trained_model.pkl', 'wb'))\n",
    "        timeToFit =  time.time()-start\n",
    "\n",
    "        toappend = evaluate(X_test,y_test, model, \"combo\", timeToFit)\n",
    "\n",
    "        metrics = pd.concat([metrics, toappend])\n",
    "        print(\"appended metrics\")\n",
    "\n",
    "        losscurve(history)\n",
    "        acccurve(history)\n",
    "    metrics.to_pickle('metrics.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69d05310",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def split_train_scale_eval(model, epochsN, func1, BATCH=32):\n",
    "\n",
    "    metrics = pd.DataFrame({'model':[],'acc':[], 'tpr':[], 'tnr':[], 'prec':[], 'fpr':[], 'roc_auc':[], 'f1':[],'time':[]},\n",
    "                        columns=['model','acc', 'tpr', 'tnr', 'prec','fpr','roc_auc','f1','time'])\n",
    "    \n",
    "    \"\"\"change seed to 40 from 42\"\"\"\n",
    "    gss = GroupShuffleSplit(n_splits=1, train_size=.7, random_state=42)\n",
    "    \n",
    "    for train_index, test_index in gss.split(X, y, ids):\n",
    "    \n",
    "        X_train = np.array(list(itemgetter(*train_index)(X)))\n",
    "        X_test = np.array(list(itemgetter(*test_index)(X)))\n",
    "        y_train = np.array(list(itemgetter(*train_index)(y)))\n",
    "        y_test = np.array(list(itemgetter(*test_index)(y)))\n",
    "    \n",
    "        X_train, X_test = scaling(X_train, X_test, func1)\n",
    "        \n",
    "        print(f\"shape of x_train: {np.shape(X_train)}, type: {type(X_train)}\")\n",
    "        print(f\"shape of x_test: {np.shape(X_test)}, type: {type(X_test)}\")\n",
    "        \n",
    "        \n",
    "        if (len(X_train)+len(X_test) != len(X)):\n",
    "            raise TypeError(\"train and test sets don't add up!\")\n",
    "        \n",
    "        #get time for start of training. Used in evaluate()\n",
    "        start = time.time()\n",
    "        \n",
    "        model.compile(loss='binary_crossentropy',\n",
    "            optimizer='adam', metrics = ['accuracy'])\n",
    "\n",
    "        history = model.fit(X_train, y_train,    \n",
    "                    epochs=epochsN,\n",
    "                    batch_size = BATCH,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    shuffle=True,        \n",
    "                    verbose=1)\n",
    "        print(\"fitted\")\n",
    "        timeToFit =  time.time()-start\n",
    "\n",
    "        toappend = evaluate(X_test,y_test, model, \"combo\", timeToFit)\n",
    "\n",
    "        metrics = pd.concat([metrics, toappend])\n",
    "        print(\"appended metrics\")\n",
    "\n",
    "        losscurve(history)\n",
    "        acccurve(history)\n",
    "        metrics.to_pickle('metrics.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10636999",
   "metadata": {},
   "outputs": [],
   "source": [
    "def foo():\n",
    "        pass\n",
    "\n",
    "def scaling(X_TRAIN,X_TEST, func1):    \n",
    "\n",
    "    # get to 2d shape\n",
    "    X_TRAIN = np.reshape(X_TRAIN, (X_TRAIN.shape[0], X_TRAIN.shape[1]))\n",
    "    X_TEST = np.reshape(X_TEST, (X_TEST.shape[0], X_TEST.shape[1]))\n",
    "\n",
    "    #replace -2 w/ NaN to be ignored in scaling\n",
    "    X_TRAIN = np.where(X_TRAIN==-2, np.nan, X_TRAIN)\n",
    "    X_TEST = np.where(X_TEST==-2, np.nan, X_TEST)\n",
    "\n",
    "    print('TRAIN SHAPE: ' + str(np.shape(X_TRAIN)))\n",
    "    print('TEST SHAPE: ' + str(np.shape(X_TEST)))\n",
    "    \n",
    "    #scale on global level\n",
    "    scaler = func1().fit(X_TRAIN)\n",
    "    X_TRAIN = scaler.transform(X_TRAIN)\n",
    "    #use scaler fit on TRAIN to scale TEST to avoid data leakage\n",
    "    X_TEST = scaler.transform(X_TEST)\n",
    "\n",
    "    #replace NaN w/ -2 to be ignored in masking \n",
    "    np.nan_to_num(X_TRAIN, copy=False, nan=-2)\n",
    "    np.nan_to_num(X_TEST, copy=False, nan=-2)\n",
    "\n",
    "    #get back to 3d shape\n",
    "    X_TRAIN = np.reshape(X_TRAIN, (X_TRAIN.shape[0], X_TRAIN.shape[1],1))\n",
    "    X_TEST = np.reshape(X_TEST, (X_TEST.shape[0], X_TEST.shape[1],1))\n",
    "    \n",
    "    print('TRAIN SHAPE: ' + str(np.shape(X_TRAIN)))\n",
    "    print('TEST SHAPE: ' + str(np.shape(X_TEST)))\n",
    "    \n",
    "    return X_TRAIN, X_TEST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80f8ff02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-21T09:40:36.112352Z",
     "iopub.status.busy": "2023-04-21T09:40:36.111559Z",
     "iopub.status.idle": "2023-04-21T09:40:37.301080Z",
     "shell.execute_reply": "2023-04-21T09:40:37.300212Z",
     "shell.execute_reply.started": "2023-04-21T09:40:36.112297Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def stratKfold_eval(modelname, EPOCHS, BUILDMODEL, BATCH=32):\n",
    "\n",
    "\n",
    "    metrics = pd.DataFrame({'model':[],'acc':[], 'tpr':[], 'tnr':[], 'prec':[], 'fpr':[], 'roc_auc':[], 'f1':[],'time':[]}\n",
    "                           ,columns=['model','acc', 'tpr', 'tnr', 'prec','fpr','roc_auc','f1','time'])\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "    #y_str is y not Onehotencoded\n",
    "    for i, (train_index, test_index) in enumerate(skf.split(X, y_str)):\n",
    "            print(f\"Fold {i}:\")\n",
    "            print('***')\n",
    "            print([X[111:116,:5].flatten(), y[111:116,:].flatten(),y_str[111:116]])\n",
    "            print('***')\n",
    "            print(f\"  Train: index={train_index}\")\n",
    "            print(f\"  Test:  index={test_index}\")\n",
    "            X_train = np.array(list(itemgetter(*train_index)(X)))\n",
    "            X_test = np.array(list(itemgetter(*test_index)(X)))\n",
    "            y_train = np.array(list(itemgetter(*train_index)(y)))\n",
    "            y_test = np.array(list(itemgetter(*test_index)(y)))\n",
    "\n",
    "            print(f\"shape of x_train: {np.shape(X_train)}, type: {type(X_train)}\")\n",
    "            print(f\"shape of x_test: {np.shape(X_test)}, type: {type(X_test)}\")\n",
    "            \n",
    "            \n",
    "            if (len(X_train)+len(X_test) != len(X)):\n",
    "                raise TypeError(\"train and test sets don't add up!\")\n",
    "\n",
    "            \"\"\"build model again at each iteration to reset weigths\"\"\"\n",
    "            model = BUILDMODEL()\n",
    "\n",
    "            #get time for start of training. Used in evaluate()\n",
    "            start = time.time()\n",
    "\n",
    "            model.compile(loss='binary_crossentropy',\n",
    "                optimizer='adam', metrics = ['accuracy'])\n",
    "\n",
    "            history = model.fit(X_train, y_train,    \n",
    "                        epochs=EPOCHS,\n",
    "                        batch_size = BATCH,\n",
    "                        validation_data=(X_test, y_test),\n",
    "                        shuffle=True,        \n",
    "                        verbose=1)\n",
    "            print(\"fitted\")\n",
    "            timeToFit =  time.time()-start\n",
    "\n",
    "            toappend = evaluate(X_test,y_test, model, modelname, timeToFit)\n",
    "\n",
    "            metrics = pd.concat([metrics, toappend])\n",
    "            print(\"appended metrics\")\n",
    "\n",
    "            #losscurve(history)\n",
    "            #acccurve(history)\n",
    "    \n",
    "    metrics.to_pickle('final_metrics.pkl')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "528df852",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-20T07:08:41.128620Z",
     "iopub.status.busy": "2023-04-20T07:08:41.127750Z",
     "iopub.status.idle": "2023-04-20T07:08:41.144905Z",
     "shell.execute_reply": "2023-04-20T07:08:41.143983Z",
     "shell.execute_reply.started": "2023-04-20T07:08:41.128560Z"
    },
    "id": "fe73d4e4-be35-425c-bea4-7167655cc2f7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#func is modelbuilding function\n",
    "def logoloopMask(PATH, func):\n",
    "\n",
    "    metrics = pd.DataFrame({'model':[],'acc':[], 'tpr':[], 'tnr':[], 'prec':[], 'fpr':[], 'roc_auc':[], 'f1':[],'time':[]},\n",
    "                        columns=['model','acc', 'tpr', 'tnr', 'prec','fpr','roc_auc','f1','time'])\n",
    "    history = []\n",
    "    \n",
    "    logo = LeaveOneGroupOut()\n",
    "    logo.get_n_splits(groups=ids)\n",
    "\n",
    "    for train_index, test_index in logo.split(X, y, ids):\n",
    "        #print(datetime.datetime.now())\n",
    "        \n",
    "        X_train = np.array(list(itemgetter(*train_index)(X)))\n",
    "        X_test = np.array(list(itemgetter(*test_index)(X)))\n",
    "        y_train = np.array(list(itemgetter(*train_index)(y)))\n",
    "        y_test = np.array(list(itemgetter(*test_index)(y)))\n",
    "\n",
    "        #y_train_all.append(y_train)\n",
    "        #y_test_all.append(y_test)\n",
    "        print(X_train.shape)\n",
    "        print(y_train.shape)\n",
    "        print(f\"shape of y: {np.shape(y_train)}, type of y: {type(y_train)}\")\n",
    "        print(f\"shape of x: {np.shape(X_train)}, type of y: {type(y_train)}\")\n",
    "\n",
    "        #Changing name of model to ID of test-individual\n",
    "        testID =  np.array(list(itemgetter(*test_index)(ids)))[0]\n",
    "        \n",
    "        #get time for start of training. Used in evaluate()\n",
    "        start = time.time()\n",
    "        \n",
    "        \"\"\"build model again at each iteration to reset weigths\"\"\"\n",
    "        model = func()\n",
    "        \n",
    "        \n",
    "        history = model.fit(X_train, y_train,    \n",
    "                    epochs=50,\n",
    "                    batch_size = 128,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    verbose=1,)\n",
    "        print(\"fitted\")\n",
    "        \n",
    "        #innerPath='results/logo/'+testID)\n",
    "        innerPath=PATH+testID\n",
    "\n",
    "\n",
    "        try:\n",
    "            os.mkdir(startdir+innerPath)\n",
    "            print('made dir '+testID)\n",
    "        except OSError as error:\n",
    "            print('dir already exists')\n",
    "            pass                \n",
    "       \n",
    "        os.chdir(startdir+innerPath)\n",
    "\n",
    "        timeToFit =  time.time()-start\n",
    "        toappend = evaluate(X_test,y_test, model, testID , timeToFit)\n",
    "        toappend.to_pickle('metrics.pkl')\n",
    "\n",
    "        metrics = pd.concat([metrics, toappend])\n",
    "        print(\"appended metrics\")\n",
    "    \n",
    "        losscurve(history)\n",
    "        acccurve(history)\n",
    "        \n",
    "    os.chdir(startdir+PATH)    \n",
    "    metrics.to_pickle('final_metrics.pkl')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a289639",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pruning(X_TEST,y_TEST,ids_TEST, ANNOT):\n",
    "    print(f'N windows in total is {len(annot)}  \\n')\n",
    "    other = np.where( ANNOT == 'Other')\n",
    "    print('***indices of ***')\n",
    "    print(np.where( ANNOT == 'Other'))\n",
    "    #print(other)\n",
    "    annot = np.delete(ANNOT, other)\n",
    "    X_test = np.delete(X_TEST, other,0)\n",
    "    y_test = np.delete(y_TEST, other,0)\n",
    "    ids_test = np.delete(ids_TEST, other)\n",
    "    print(f'N windows after removal is {len(annot)}  \\n')  \n",
    "    \n",
    "    #confirm that arrays are same length and correct shape\n",
    "    print('***ids***')\n",
    "    print(np.shape(ids_TEST))\n",
    "    print('***X***')\n",
    "    print(np.shape(X_TEST))\n",
    "    print('***Y***')\n",
    "    print(np.shape(y_TEST))\n",
    "    print('***annot***')\n",
    "    print(np.shape(ANNOT)) \n",
    "    return X_TEST,y_TEST,ids_TEST, ANNOT\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
